---
title: "Causal Inference"
subtitle: "Topics"
format: 
   revealjs:
    embed-resources: true
    theme: serif
    slide-level: 3
    slide-number: true
    show-slide-number: all
    preview-links: auto
    number-sections: true
    link-color: orange
author: "Macartan Humphreys"
bibliography: bib.bib
---

```{r, include = FALSE}
source("setup.R")
```

# Observational designs {#citopics}

*Introduction to observational strategies and more advanced  topics*

## Outline

* LATE
* Diff in Diff
* RDD

## Noncompliance and the LATE estimand {#LATE}

```{r, echo = FALSE}
model <- make_model("Z -> X -> Y; X <-> Y") 
model |> plot()
```

### Local Average Treatment Effects {.smaller}

Sometimes you give a medicine but only a nonrandom sample of people actually try to use it. Can you still estimate the medicine's effect?

|     | X=0                           | X=1                           |
|-----|-------------------------------|-------------------------------|
| Z=0 | $\overline{y}_{00}$ ($n_{00}$) | $\overline{y}_{01}$ ($n_{01}$) |
| Z=1 | $\overline{y}_{10}$ ($n_{10}$) | $\overline{y}_{11}$ ($n_{11}$) |

Say that people are one of 3 types:

1.  $n_a$ "always takers" have $X=1$ no matter what and have average outcome $\overline{y}_a$
2.  $n_n$ "never takers" have $X=0$ no matter what with outcome $\overline{y}_n$
3.  $n_c$ "compliers have" $X=Z$ and average outcomes $\overline{y}^1_c$ if treated and $\overline{y}^0_c$ if not.

### Local Average Treatment Effects  {.smaller}

Sometimes you give a medicine but only a non random sample of people actually try to use it. Can you still estimate the medicine's effect?

|     | X=0                           | X=1                           |
|-----|-------------------------------|-------------------------------|
| Z=0 | $\overline{y}_{00}$ ($n_{00}$) | $\overline{y}_{01}$ ($n_{01}$) |
| Z=1 | $\overline{y}_{10}$ ($n_{10}$) | $\overline{y}_{11}$ ($n_{11}$) |

We can figure something about types:

|          | $X=0$                  | $X=1$                  |                                                                                                        |
|----------|:----------------------:|:----------------------:|
| $Z=0$ | $\frac{\frac{1}{2}n_c}{\frac{1}{2}n_c + \frac{1}{2}n_n} \overline{y}^0_{c}+\frac{\frac{1}{2}n_n}{\frac{1}{2}n_c + \frac{1}{2}n_n} \overline{y}_{n}$ | $\overline{y}_{a}$                                                                                                                                  |
| $Z=1$ | $\overline{y}_{n}$                                                                                                                                  | $\frac{\frac{1}{2}n_c}{\frac{1}{2}n_c + \frac{1}{2}n_a} \overline{y}^1_{c}+\frac{\frac{1}{2}n_a}{\frac{1}{2}n_c + \frac{1}{2}n_a} \overline{y}_{a}$ |

### Local Average Treatment Effects  {.smaller}

You give a medicine to 50% but only a non random sample of people actually try to use it. Can you still estimate the medicine's effect?

|         | $X=0$                                                                           | $X=1$                                                                             |
|---------|:---------------------:|:---------------------:|
| $Z=0$ | $\frac{n_c}{n_c + n_n} \overline{y}^0_{c}+\frac{n_n}{n_c + n_n} \overline{y}_n$ | $\overline{y}_{a}$ |
| \(n\) | ($\frac{1}{2}(n_c + n_n)$)                                                      | ($\frac{1}{2}n_a$)  |
| $Z=1$ | $\overline{y}_{n}$                  | $\frac{n_c}{n_c + n_a} \overline{y}^1_{c}+\frac{n_a}{n_c + n_a} \overline{y}_{a}$ |
| \(n\) | ($\frac{1}{2}n_n$)                                                              | ($\frac{1}{2}(n_a+n_c)$) |                 

Key insight: the contributions of the $a$s and $n$s  are the *same* in the $Z=0$ and $Z=1$ groups so if you difference you are left with the *changes* in the contributions of the $c$s. 


### Local Average Treatment Effects  {.smaller}

Average in $Z=0$ group: $\frac{{n_c} \overline{y}^0_{c}+ \left(n_{n}\overline{y}_{n} +{n_a} \overline{y}_a\right)}{n_a+n_c+n_n}$

Average in $Z=1$ group: $\frac{{n_c} \overline{y}^1_{c} + \left(n_{n}\overline{y}_{n} +{n_a} \overline{y}_a \right)}{n_a+n_c+n_n}$

So, the *difference* is the ITT:  $({\overline{y}^1_c-\overline{y}^0_c})\frac{n_c}{n}$

Last step:

$$ITT = ({\overline{y}^1_c-\overline{y}^0_c})\frac{n_c}{n}$$

$$\leftrightarrow$$ 

$$LATE = \frac{ITT}{\frac{n_c}{n}}= \frac{\text{Intent to treat effect}}{\text{First stage effect}}$$


### The good and the bad of LATE  {.smaller}

-   You get a well-defined estimate even when there is non-random take-up
-   May sometimes be used to assess mediation or knock-on effects
-   But:
    -   You need assumptions (monotonicity and the exclusion restriction -- *where were these used above*?)
    -   Your estimate is only for a subpopulation
    -   The subpopulation is not chosen by you and is unknown
    -   Different encouragements may yield different estimates since they may encourage different subgroups

### Declaration

```{r}
declaration_iv <-
  declare_model(
    N = 100, 
    U = rnorm(N),
    potential_outcomes(D ~ if_else(Z + U > 0, 1, 0), 
                       conditions = list(Z = c(0, 1))), 
    potential_outcomes(Y ~ 0.1 * D + 0.25 + U, 
                       conditions = list(D = c(0, 1))),
    complier = D_Z_1 == 1 & D_Z_0 == 0
  ) + 
  declare_inquiry(ATE = mean(Y_D_1 - Y_D_0), 
                  LATE = mean(Y_D_1[complier] - Y_D_0[complier])) + 
  declare_assignment(Z = complete_ra(N, prob = 0.5)) +
  declare_measurement(D = reveal_outcomes(D ~ Z),
                      Y = reveal_outcomes(Y ~ D)) + 
  declare_estimator(Y ~ D, inquiry = "ATE", label = "OLS")  +
  declare_estimator(Y ~ D | Z, .method = iv_robust, inquiry = "LATE",
                    label = "IV")  

```

### Diagnosis

```{r, echo = FALSE}

if(run)
  diagnose_design(declaration_iv, sims = 5000) |>
  write_rds("saved/diagnosis_iv.rds")

read_rds("saved/diagnosis_iv.rds") |> reshape_diagnosis() |>
  select(Inquiry, Estimator, 'Mean Estimand', 'Mean Estimate', 'Bias', "RMSE") |>
  kable()

```

Note:

* The estimands *might* be the same
* The estimators might both be biased
* And in opposite directions

## Diff in diff

Key idea: the evolution of units in the control group allow you to impute what the evolution of units in the treatment group *would have been* had they not been treated


### Logic {.smaller}

We have group $A$ that enters treatment at some point and group $B$ that never does

The estimate:

$$\hat\tau = (\mathbb{E}[Y^A | post] - \mathbb{E}[Y^A | pre]) -(\mathbb{E}[Y^B | post] - \mathbb{E}[Y^B | pre])$$
(how different is the change in $A$ compared to the change in $B$?)

can be written:

$$\hat\tau = (\mathbb{E}[Y_1^A | post] - \mathbb{E}[Y_0^A | pre]) -(\mathbb{E}[Y_0^B | post] - \mathbb{E}[Y_0^B | pre])$$

### Logic {.smaller}
Cleaning up

$$\hat\tau = (\mathbb{E}[Y_1^A | post] - \mathbb{E}[Y_0^A | pre]) -(\mathbb{E}[Y_0^B | post] - \mathbb{E}[Y_0^B | pre])$$

$$\hat\tau = (\mathbb{E}[Y_1^A | post] - \mathbb{E}[Y_0^A | post]) + ((\mathbb{E}[Y_0^A | post] - \mathbb{E}[Y_0^A | pre]) -(\mathbb{E}[Y_0^B | post] - \mathbb{E}[Y_0^B | pre]))$$

$$\hat\tau_{ATT} = \tau_{ATT} + \text{Difference in trends}$$



### Simplest case

```{r}
n_units <- 2
design <- 
  declare_model(
    unit = add_level(N = n_units, I = 1:N),
    time = add_level(N = 6, T = 1:N, nest = FALSE),
    obs = cross_levels(by = join_using(unit, time))) +
  declare_model(potential_outcomes(Y ~ I + T^.5 + Z*T)) +
  declare_assignment(Z = 1*(I>(n_units/2))*(T>3)) +
  declare_measurement(Y = reveal_outcomes(Y~Z)) + 
  declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0),
                  ATT = mean(Y_Z_1[Z==1] - Y_Z_0[Z==1])) +
  declare_estimator(Y ~ Z, label = "naive") + 
  declare_estimator(Y ~ Z + I, label = "FE1") + 
  declare_estimator(Y ~ Z + as.factor(T), label = "FE2") + 
  declare_estimator(Y ~ Z + I + as.factor(T), label = "FE3")  


```

### Diagnosis {.smaller}

Here only the two way fixed effects is unbiased and only for the ATT.

The ATT here is averaging over effects for treated units (later periods only). We *know nothing* about the size of effects in earlier periods when all units are in control!



```{r, warning = FALSE, eval = FALSE}
design |> diagnose_design() 
```

```{r, warning = FALSE, echo = FALSE}
if(run)
  design |> 
  redesign(n_units = 10) |>  
  diagnose_design(bootstrap_sims = 0)  |>
  write_rds("saved/dd1.rds") 

read_rds("saved/dd1.rds") |>
  reshape_diagnosis() |> select(Inquiry, Estimator, Bias) |> kable()

```

### The classic graph

```{r}
design |> 
  draw_data() |>
  ggplot(aes(T, Y, color = unit)) + geom_line() +
       geom_point(aes(T, Y_Z_0)) + theme_bw()
```


### Extends to multiple units easily {.smaller}


```{r, warning = FALSE, eval = FALSE}
design |> redesign(n_units = 10) |> diagnose_design() 
```

```{r, warning = FALSE, echo = FALSE}
if(run)
  design |> 
  redesign(n_units = 10) |>  
  diagnose_design(bootstrap_sims = 0)  |>
  write_rds("saved/dd10.rds")

read_rds("saved/dd10.rds") |>
  reshape_diagnosis() |> select(Inquiry, Estimator, Bias) |> kable()

```

### Extends to multipe units easily


```{r}
design |> 
  redesign(n_units = 10) |>  
  draw_data() |> 
  ggplot(aes(T, Y, color = unit)) + geom_line() +
       geom_point(aes(T, Y_Z_0)) + theme_bw()
```

### In practice

* Need to defend parallel trends
* Most typically using an *event study*
* Sometimes: report balance between treatment and control groups in covariates
* Placebo leads and lags


### Heterogeneity {.smaller}

* Things get much more complicated when there is (a) heterogeneous timing in treatment take up and (b) heterogeneous effects


*  It's only recently been appreciated how tricky things can get

*  But we already have an intuition from our analysis of trials with heterogeneous assignment and heterogeneous effects: 

  * in such cases fixed effects analysis weights stratum level treatment effects by the variance in assignment to treatment
  * something similar here 

### Staggared assignments {.smaller}

Just two units assigned at different times:

```{r}
trend = 0

design <- 
  declare_model(
    unit = add_level(N = 2, ui = rnorm(N), I = 1:N),
    time = add_level(N = 6, ut = rnorm(N), T = 1:N, nest = FALSE),
    obs = cross_levels(by = join_using(unit, time))) +
  declare_model(
    potential_outcomes(Y ~ trend*T + (1+Z)*(I == 2))) +
  declare_assignment(Z = 1*((I == 1) * (T>3) + (I == 2) * (T>5))) +
  declare_measurement(Y = reveal_outcomes(Y~Z), 
                      I_c = I - mean(I)) +
  declare_inquiry(mean(Y_Z_1 - Y_Z_0)) +
  declare_estimator(Y ~ Z, label = "1. naive") + 
  declare_estimator(Y ~ Z + I, label = "2. FE1") + 
  declare_estimator(Y ~ Z + as.factor(T), label = "3. FE2") + 
  declare_estimator(Y ~ Z + I + as.factor(T), label = "4. FE3") + 
  declare_estimator(Y ~ Z*I_c + as.factor(T), label = "5. Sat")  
```


### Staggared assignments diagnosis {.smaller}


```{r, message = FALSE, echo = FALSE}
diagnose_design(design, sims = 1) |>
  reshape_diagnosis() |> select(Estimator, 'Mean Estimand', 'Mean Estimate') |> kable()
```




See causal infernece slides for intuitions on what is happening here.


## Regression discontintuity

Errors and diagnostics



### Intuition

* The core idea in an RDD design is that if a decision rule assigns units that are *almost identical* to each other to treatment and control conditions then we can infer effects *for those cases* by looking *at those cases*.


See excellent introduction:  @lee2010regression

### Intuition  {.smaller}


* Kids born on 31 August start school a year younger than kids born on 1 September: does starting younger help or hurt?

* Kids born on 12 September 1983 are more likely to register Republican than kids born on  10 September 1983: can this identify the effects of registration on long term voting? 

* A district in which Republicans got 50.1% of the vote get a Republican representative while districts in which Republicans got 49.9% of the vote do not: does having a Republican representative make a difference for these districts?



### Argument for identification {.smaller}

Setting:

* Typically the decision is based on a value on a "running variable", $X$. e.g. Treatment if $X > 0$
* The estimand is $\mathbb{E}[Y(1) - Y(0)|X=0]$

Two arguments:

1. Continuity: $\mathbb{E}[Y(1)|X=x]$ and $\mathbb{E}[Y(0)|X=x]$ are continuous (at $x=0$) in $x$: so  $\lim_{\hat x \rightarrow 0}\mathbb{E}[Y(0)|X=\hat x] = \mathbb{E}[Y(0)|X=\hat 0]$

2. Local randomization: tiny things that determine exact values of $x$ are *as if* random and so we can think of a local experiment around $X=0$.


### Argument for identification  {.smaller}

Note: 

* continuity argument requires continuous $x$: granularity
* also builds off a conditional expectation function defined at $X=0$

Exclusion restriction is  *implicit* in continuity: If *something else* happens at the threshold then the conditional expectation functions jump at the thresholds

Implicit: $X$ is exogenous in the sense that units  cannot adjust $X$ in order to be on one or the other side of the threshold 

### Evidence

Typically researchers show:

1. "First stage" results: assignment to treatment does indeed jump at the threshold
2. "ITT": outcomes jump at the threshold
3.  LATE (if fuzzy / imperfect compliance) using IV

### Evidence

Typically researchers show:

In addition:

1. Arguments for no other treatments at the threshold
2. Arguments for no "sorting" at the threshold
3. Evidence for no "heaping" at the threshold (McCrary density test)

Sometimes: 

* argue for why estimates extend beyond the threshold
* exclude points *at* the threshold (!)

### Design

```{r}
library(rdss) # for helper functions
library(rdrobust)

cutoff <- 0.5
bandwidth <- 0.5

control <- function(X) {
  as.vector(poly(X, 4, raw = TRUE) %*% c(.7, -.8, .5, 1))}
treatment <- function(X) {
  as.vector(poly(X, 4, raw = TRUE) %*% c(0, -1.5, .5, .8)) + .25}

rdd_design <-
  declare_model(
    N = 1000,
    U = rnorm(N, 0, 0.1),
    X = runif(N, 0, 1) + U - cutoff,
    D = 1 * (X > 0),
    Y_D_0 = control(X) + U,
    Y_D_1 = treatment(X) + U
  ) +
  declare_inquiry(LATE = treatment(0) - control(0)) +
  declare_measurement(Y = reveal_outcomes(Y ~ D)) + 
  declare_sampling(S = X > -bandwidth & X < bandwidth) +
  declare_estimator(Y ~ D*X, term = "D", label = "lm") + 
  declare_estimator(
    Y, X, 
    term = "Bias-Corrected",
    .method = rdrobust_helper,
    label = "optimal"
  )

```

### RDD Data plotted

Note `rdrobust` implements:

* local polynomial Regression Discontinuity (RD) point estimators
* robust bias-corrected confidence intervals 

See  @calonico2014robust and related papers `? rdrobust::rdrobust`

### RDD Data plotted

```{r}
rdd_design  |> draw_data() |> ggplot(aes(X, Y, color = factor(D))) + 
  geom_point(alpha = .3) + theme_bw() +
  geom_smooth(aes(X, Y_D_0)) + geom_smooth(aes(X, Y_D_1)) + theme(legend.position = "none")

```

### RDD diagnosis

```{r, eval = FALSE, echo = TRUE}
rdd_design |> diagnose_design()
```


```{r, echo = FALSE, eval = TRUE}
if(run)
  rdd_design |> diagnose_design() |> write_rds("saved/rdd_diagnosis.rds")

read_rds("saved/rdd_diagnosis.rds") |> reshape_diagnosis() |> 
  select(Estimator, 'Mean Estimate', 'Bias', 'SD Estimate', 'Coverage') |> kable()
```

### Bandwidth tradeoff {.smaller}

```{r, echo = TRUE, eval = FALSE}
rdd_design |> 
  redesign(bandwidth = seq(from = 0.05, to = 0.5, by = 0.05)) |> 
  diagnose_designs()
```

```{r, echo = FALSE, eval = TRUE, fig.height = 2, fig.width = 8}

if(run)
rdd_design |> 
  redesign(bandwidth = seq(from = 0.1, to = 0.5, by = 0.05)) |> 
  diagnose_designs() |>
  write_rds("saved/rddbandwidth.rds")

read_rds("saved/rddbandwidth.rds")|>  tidy() |>
  filter(diagnosand %in% c("bias", "rmse", "sd_estimate")) |>
  ggplot(aes(bandwidth, estimate, color = diagnosand)) + geom_line()  + theme_bw() +
  facet_grid(~estimator)

```

* As we increase the bandwidth, the lm bias gets worse, but slowly, while the error falls. 
* The best bandwidth is relatively wide. 
* This is more true for the optimal estimator.


